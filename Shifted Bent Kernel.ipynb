{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning with Kernel Methods\n",
    "\n",
    "Here we use a variational classifier to learn a nonlinear boundary using kernel methods.\n",
    "\n",
    "The variational circuit architecture is specified by [Farhi and Neven (2018)](https://arxiv.org/abs/1802.06002). \n",
    "The kernel map is specified by [Havlicek et al (2018)](https://arxiv.org/abs/1804.11326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import NesterovMomentumOptimizer\n",
    "\n",
    "from scipy.stats import unitary_group\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev = qml.device('default.qubit', wires=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with a subgate in our kernel map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U_phi(x):\n",
    "    #print(x, np.shape(x))\n",
    "    # x3 := (pi - x1)(pi - x2)\n",
    "    x_0, x_1, x_2 = x[0], x[1], x[2]\n",
    "    #print(x_0, x_1, x_2)\n",
    "        \n",
    "    qml.RZ( x_0 , wires=0)\n",
    "    qml.RZ( x_1 , wires=1)\n",
    "    \n",
    "    qml.CNOT(wires=[0,1])\n",
    "    qml.RZ(x_2,wires=1)\n",
    "    qml.CNOT(wires=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our kernel map is specified by U_phi interlaced twice with hadamards on all qubits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featuremap(x):\n",
    "    for i in range(2):\n",
    "        qml.Hadamard(wires=0)\n",
    "        qml.Hadamard(wires=1)\n",
    "        U_phi(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. We are ready to define a layer of our variational circuit. This is the component of the circuit that is tunable by learned weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer(W): # 6 weights are specified at each layer\n",
    "    \n",
    "    # euler angles\n",
    "    qml.Rot(W[0, 0], W[0, 1], W[0, 2], wires=0)\n",
    "    qml.Rot(W[1, 0], W[1, 1], W[1, 2], wires=1)\n",
    "\n",
    "    qml.CNOT(wires=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fully specify our variational circuit. This circuit\n",
    "\n",
    "1. maps the classical data using the above feature map\n",
    "2. applies a specified number of the above variational layers which are parameterized by tunable weights\n",
    "3. Measures the expectation of Z on both qubits so as to compute expected parity\n",
    "\n",
    "Hence, we define two circuits for each expected Z on a qubit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@qml.qnode(dev)\n",
    "def circuit1(weights, x):\n",
    "\n",
    "    featuremap(x)\n",
    "\n",
    "    for W in weights:\n",
    "        layer(W)\n",
    "\n",
    "    return qml.expval.PauliZ(wires=0)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def circuit2(weights, x):\n",
    "\n",
    "    featuremap(x)\n",
    "\n",
    "    for W in weights:\n",
    "        layer(W)\n",
    "\n",
    "    return qml.expval.PauliZ(wires=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more modification: we add a bias and specify multiplying the expectations of the two circuits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variational_classifier(var, x): # x is a keyword argument -> fixed (not trained)\n",
    "    weights = var[0]\n",
    "    bias = var[1]\n",
    "\n",
    "    return circuit1(weights, x) * circuit2(weights, x) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use L2 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def square_loss(labels, predictions):\n",
    "\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss = loss + (l - p) ** 2\n",
    "    loss = loss / len(labels)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(labels, predictions):\n",
    "    #print(labels, predictions)\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        if abs(l - p) < 1e-5:\n",
    "            loss = loss + 1\n",
    "    loss = loss / len(labels)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(var, X, Y):\n",
    "\n",
    "    predictions = [variational_classifier(var, x) for x in X]\n",
    "    #if (len(Y) == num_data):\n",
    "    #    print(\"[(pred, label), ...]: \", list(zip(predictions, Y)))\n",
    "    return square_loss(Y, predictions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a random element of SU(4), we generate a random element of U(4) and divide appropriately to scale the determinant to +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random unitary:  [[ 0.56480469+0.02828828j  0.00919685-0.54666223j  0.17628252-0.49642827j\n",
      "   0.2758558 -0.16630779j]\n",
      " [ 0.76026959-0.00843403j  0.14776823+0.17506096j -0.10832053+0.47307077j\n",
      "  -0.35248134-0.09830992j]\n",
      " [ 0.19169964+0.02404574j -0.6268324 +0.26471267j  0.31711591+0.29431852j\n",
      "   0.54700278+0.11525388j]\n",
      " [-0.25430813+0.0107177 j -0.06982527-0.42511976j  0.28338993+0.46846389j\n",
      "  -0.06125777-0.6678992 j]]\n",
      "det:  (1.0000000000000002-8.326672684688678e-17j)\n"
     ]
    }
   ],
   "source": [
    "random_U = unitary_group.rvs(4)\n",
    "random_U = random_U / (np.linalg.det(random_U) ** (1/4)) # so that det = 1\n",
    "\n",
    "\n",
    "print(\"random unitary: \", random_U)\n",
    "print(\"det: \", np.linalg.det(random_U))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We label classical data by\n",
    "\n",
    "1. applying the above feature map\n",
    "2. applying the fixed random unitary\n",
    "3. returning expectedparity\n",
    "\n",
    "If the absolute value of the expected parity is above a fixed threshold, then we can use this point and label by sign. If not, then we continue generating more random samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@qml.qnode(dev)\n",
    "def data_label_1(x):\n",
    "    #print(u)\n",
    "    #print(\"label the following:\", x)\n",
    "    featuremap(x)\n",
    "    qml.QubitUnitary(random_U, wires=[0,1])\n",
    "    \n",
    "    return qml.expval.PauliZ(wires=0)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def data_label_2(x):\n",
    "    #print(\"label the following:\", x)\n",
    "    featuremap(x)\n",
    "    qml.QubitUnitary(random_U, wires=[0,1])\n",
    "    \n",
    "    return qml.expval.PauliZ(wires=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(thresh):\n",
    "    #thresh = 0.3\n",
    "\n",
    "    X = np.array([])\n",
    "    Y = np.array([])\n",
    "    ctr = 0 # num valid data pts\n",
    "    maxval = 0.0\n",
    "    minval = 0.0\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    while ctr < 40:\n",
    "        x = np.random.rand(2) * 2 * np.pi\n",
    "        x = np.append(x, (np.pi - x[0]) * (np.pi - x[1]))\n",
    "        y_1 = data_label_1(x)\n",
    "        y_2 = data_label_2(x)\n",
    "        #print(y_1, y_2, y_1 * y_2)\n",
    "        if (np.abs(y_1 * y_2) > maxval):\n",
    "            maxval = y_1 * y_2\n",
    "            #print(\"new max separation: \", maxval)\n",
    "        elif (y_1 * y_2 < minval):\n",
    "            minval = y_1 * y_2\n",
    "            #print(\"new min separation: \", minval)\n",
    "\n",
    "        if y_1 * y_2 > thresh:\n",
    "            Y = np.append(Y, +1)\n",
    "            X = np.append(X, x)\n",
    "            ctr += 1\n",
    "            #print(\"+1\")\n",
    "        elif y_1 * y_2 < -1 * thresh:\n",
    "            Y = np.append(Y, -1)\n",
    "            X = np.append(X, x)\n",
    "            ctr += 1\n",
    "            #print(\"-1\")\n",
    "            \n",
    "    X = X.reshape(-1, 3)\n",
    "    print(\"Data: \", list(zip(X, Y)))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can divide our training and test data randomly. We specify that half is used for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_train_test(X, Y):\n",
    "    global num_data\n",
    "    num_data = len(Y)\n",
    "    global num_train\n",
    "    num_train = int(0.5 * num_data)\n",
    "\n",
    "    print(\"size data, size train: \", num_data, num_train)\n",
    "\n",
    "    index = np.random.permutation(range(num_data))\n",
    "    X_train = X[index[:num_train]]\n",
    "    Y_train = Y[index[:num_train]]\n",
    "\n",
    "    X_test = X[index[num_train:]]\n",
    "    Y_test = Y[index[num_train:]]\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qubits = 2\n",
    "num_layers = 6\n",
    "var_init = (0.01 * np.random.randn(num_layers, num_qubits, 3), 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn the weights, we use a batch (size 5) momentum optimizer--Nesterov. We iterate 100 times for the specified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(thresh):\n",
    "    X, Y = gen_data(thresh)\n",
    "    X_train, Y_train, X_test, Y_test = divide_train_test(X, Y)\n",
    "\n",
    "    opt = NesterovMomentumOptimizer(0.01)\n",
    "    batch_size = 5\n",
    "\n",
    "    # train the variational classifier\n",
    "    var = var_init\n",
    "    \n",
    "    test_accuracies = []\n",
    "    train_accuracies = []\n",
    "    costs = []\n",
    "    for it in range(100):\n",
    "\n",
    "        # Update the weights by one optimizer step\n",
    "        batch_index = np.random.randint(0, num_train, (batch_size, ))\n",
    "        X_train_batch = X_train[batch_index]\n",
    "        Y_train_batch = Y_train[batch_index]\n",
    "        var = opt.step(lambda v: cost(v, X_train_batch, Y_train_batch), var)\n",
    "\n",
    "        # Compute predictions on train and validation set\n",
    "        predictions_train = [np.sign(variational_classifier(var, f)) for f in X_train]\n",
    "        predictions_test = [np.sign(variational_classifier(var, f)) for f in X_test]\n",
    "\n",
    "        # Compute accuracy on train and validation set\n",
    "        acc_train = accuracy(Y_train, predictions_train)\n",
    "        acc_test = accuracy(Y_test, predictions_test)\n",
    "        \n",
    "        # Compute cost on all samples\n",
    "        c = cost(var, X, Y)\n",
    "        \n",
    "        costs.append(c)\n",
    "        test_accuracies.append(acc_train)\n",
    "        train_accuracies.append(acc_test)\n",
    "        \n",
    "        print(\"Iter: {:5d} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc validation: {:0.7f} \"\n",
    "              \"\".format(it+1, c, acc_train, acc_test))\n",
    "        \n",
    "    return train_accuracies, test_accuracies, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We experiment with various data thresholds to determine how robust this model truly is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold:  0.0\n",
      "Data:  [(array([3.44829694, 4.49366732, 0.4146871 ]), 1.0), (array([3.78727399, 3.42360201, 0.18208818]), 1.0), (array([ 2.66190161,  4.0582724 , -0.43972306]), -1.0), (array([ 2.74944154,  5.60317502, -0.96531227]), -1.0), (array([ 6.0548717 ,  2.40923412, -2.13356478]), -1.0), (array([4.97455513, 3.32314479, 0.33277825]), 1.0), (array([3.56912924, 5.8156952 , 1.14327667]), 1.0), (array([0.44633272, 0.54744954, 6.99189002]), -1.0), (array([ 0.12703594,  5.23150478, -6.30015864]), -1.0), (array([4.88930306, 5.46644755, 4.0631731 ]), 1.0), (array([6.14884039, 5.02126135, 5.65262943]), -1.0), (array([ 2.89956035,  4.90420945, -0.42661021]), -1.0), (array([ 0.74314013,  4.02074236, -2.10859883]), 1.0), (array([ 0.90071527,  5.93552986, -6.2608707 ]), -1.0), (array([ 3.27886971,  2.60539781, -0.07360725]), 1.0), (array([ 1.66225193,  4.86465374, -2.54899443]), 1.0), (array([ 2.86607707,  3.57157584, -0.11846707]), -1.0), (array([ 0.1180598 ,  3.88071828, -2.23477062]), 1.0), (array([3.84591085, 3.87631062, 0.51747524]), -1.0), (array([5.92974406, 4.28400329, 3.18521381]), 1.0), (array([2.25885476, 2.74595275, 0.34924633]), 1.0), (array([ 4.38334608,  0.3784078 , -3.43119426]), 1.0), (array([4.18941883, 4.21374201, 1.12342616]), -1.0), (array([1.32187262, 0.81006782, 4.24272246]), -1.0), (array([1.98189478, 2.28526217, 0.99308464]), 1.0), (array([ 3.58265197,  2.75581459, -0.17015101]), 1.0), (array([ 6.21013598,  0.64116646, -7.67266612]), -1.0), (array([1.31241136, 1.01353759, 3.8925985 ]), -1.0), (array([ 4.10360063,  1.59147808, -1.4912226 ]), -1.0), (array([2.929917  , 1.53577129, 0.33991329]), -1.0), (array([0.99883535, 0.69350747, 5.24565241]), -1.0), (array([ 4.12384043,  0.86822909, -2.23300631]), 1.0), (array([1.23516341, 2.31676857, 1.57246875]), 1.0), (array([ 5.1584526 ,  0.61010531, -5.10565543]), -1.0), (array([ 5.26496313,  0.6038041 , -5.38866528]), 1.0), (array([ 6.13527576,  2.94462234, -0.58966669]), 1.0), (array([6.13717092, 3.80035648, 1.97337861]), 1.0), (array([ 4.64493006,  0.24622416, -4.35271576]), -1.0), (array([1.77692855, 0.75521727, 3.25660082]), -1.0), (array([1.86070374, 0.74598826, 3.06850311]), -1.0)]\n",
      "size data, size train:  40 20\n",
      "Iter:     1 | Cost: 1.2109868 | Acc train: 0.3500000 | Acc validation: 0.4500000 \n",
      "Iter:     2 | Cost: 1.2056718 | Acc train: 0.3500000 | Acc validation: 0.4500000 \n",
      "Iter:     3 | Cost: 1.1946175 | Acc train: 0.3500000 | Acc validation: 0.4500000 \n",
      "Iter:     4 | Cost: 1.1880957 | Acc train: 0.3500000 | Acc validation: 0.5000000 \n",
      "Iter:     5 | Cost: 1.1792460 | Acc train: 0.3500000 | Acc validation: 0.5500000 \n",
      "Iter:     6 | Cost: 1.1722361 | Acc train: 0.3000000 | Acc validation: 0.5500000 \n",
      "Iter:     7 | Cost: 1.1676888 | Acc train: 0.3000000 | Acc validation: 0.5500000 \n",
      "Iter:     8 | Cost: 1.1659294 | Acc train: 0.3000000 | Acc validation: 0.4500000 \n",
      "Iter:     9 | Cost: 1.1639392 | Acc train: 0.3000000 | Acc validation: 0.4500000 \n",
      "Iter:    10 | Cost: 1.1613333 | Acc train: 0.3000000 | Acc validation: 0.4500000 \n",
      "Iter:    11 | Cost: 1.1573012 | Acc train: 0.3500000 | Acc validation: 0.4500000 \n",
      "Iter:    12 | Cost: 1.1489657 | Acc train: 0.3000000 | Acc validation: 0.5000000 \n",
      "Iter:    13 | Cost: 1.1361739 | Acc train: 0.3000000 | Acc validation: 0.5000000 \n",
      "Iter:    14 | Cost: 1.1267185 | Acc train: 0.3000000 | Acc validation: 0.6000000 \n",
      "Iter:    15 | Cost: 1.1187139 | Acc train: 0.3000000 | Acc validation: 0.6000000 \n",
      "Iter:    16 | Cost: 1.1108472 | Acc train: 0.3500000 | Acc validation: 0.5500000 \n",
      "Iter:    17 | Cost: 1.1022188 | Acc train: 0.4000000 | Acc validation: 0.6000000 \n",
      "Iter:    18 | Cost: 1.0941938 | Acc train: 0.4500000 | Acc validation: 0.6000000 \n",
      "Iter:    19 | Cost: 1.0863286 | Acc train: 0.4500000 | Acc validation: 0.6000000 \n",
      "Iter:    20 | Cost: 1.0809904 | Acc train: 0.4500000 | Acc validation: 0.6000000 \n",
      "Iter:    21 | Cost: 1.0760177 | Acc train: 0.4500000 | Acc validation: 0.6000000 \n",
      "Iter:    22 | Cost: 1.0717301 | Acc train: 0.4000000 | Acc validation: 0.6000000 \n",
      "Iter:    23 | Cost: 1.0691276 | Acc train: 0.4500000 | Acc validation: 0.6000000 \n",
      "Iter:    24 | Cost: 1.0672302 | Acc train: 0.4500000 | Acc validation: 0.6000000 \n",
      "Iter:    25 | Cost: 1.0623843 | Acc train: 0.4500000 | Acc validation: 0.6000000 \n",
      "Iter:    26 | Cost: 1.0578723 | Acc train: 0.4500000 | Acc validation: 0.6000000 \n",
      "Iter:    27 | Cost: 1.0531617 | Acc train: 0.5500000 | Acc validation: 0.6000000 \n",
      "Iter:    28 | Cost: 1.0464287 | Acc train: 0.5500000 | Acc validation: 0.5500000 \n",
      "Iter:    29 | Cost: 1.0409820 | Acc train: 0.5500000 | Acc validation: 0.5500000 \n",
      "Iter:    30 | Cost: 1.0326692 | Acc train: 0.5000000 | Acc validation: 0.5500000 \n",
      "Iter:    31 | Cost: 1.0238982 | Acc train: 0.5000000 | Acc validation: 0.5500000 \n",
      "Iter:    32 | Cost: 1.0162147 | Acc train: 0.5000000 | Acc validation: 0.5500000 \n",
      "Iter:    33 | Cost: 1.0077078 | Acc train: 0.5000000 | Acc validation: 0.6000000 \n",
      "Iter:    34 | Cost: 0.9991452 | Acc train: 0.5000000 | Acc validation: 0.6000000 \n",
      "Iter:    35 | Cost: 0.9896414 | Acc train: 0.6000000 | Acc validation: 0.6000000 \n",
      "Iter:    36 | Cost: 0.9819892 | Acc train: 0.6000000 | Acc validation: 0.5500000 \n",
      "Iter:    37 | Cost: 0.9747347 | Acc train: 0.6000000 | Acc validation: 0.5500000 \n",
      "Iter:    38 | Cost: 0.9686913 | Acc train: 0.6000000 | Acc validation: 0.5500000 \n",
      "Iter:    39 | Cost: 0.9633015 | Acc train: 0.6500000 | Acc validation: 0.5000000 \n",
      "Iter:    40 | Cost: 0.9579468 | Acc train: 0.7000000 | Acc validation: 0.5000000 \n",
      "Iter:    41 | Cost: 0.9549896 | Acc train: 0.7000000 | Acc validation: 0.4500000 \n",
      "Iter:    42 | Cost: 0.9508966 | Acc train: 0.7000000 | Acc validation: 0.4500000 \n",
      "Iter:    43 | Cost: 0.9451717 | Acc train: 0.7000000 | Acc validation: 0.4500000 \n",
      "Iter:    44 | Cost: 0.9380771 | Acc train: 0.7000000 | Acc validation: 0.5000000 \n",
      "Iter:    45 | Cost: 0.9305741 | Acc train: 0.7000000 | Acc validation: 0.4500000 \n"
     ]
    }
   ],
   "source": [
    "thresholds = [0.0, 0.1, 0.2, 0.3]\n",
    "thresh_test_accuracies = []\n",
    "thresh_train_accuracies = []\n",
    "thresh_costs = []\n",
    "for thresh in thresholds:\n",
    "        print(\"New threshold: \", thresh)\n",
    "        trn_ac, tst_ac, costs = train_and_test(thresh)\n",
    "        thresh_train_accuracies.append(trn_ac)\n",
    "        thresh_test_accuracies.append(tst_ac)\n",
    "        thresh_costs.append(costs)\n",
    "        \n",
    "print(thresh_test_accuracies)\n",
    "print(thresh_train_accuracies)\n",
    "print(thresh_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(20,20)) \n",
    "\n",
    "num_thresh = len(thresholds)\n",
    "for i in range(num_thresh):\n",
    "    plt.subplot(num_thresh, 1, i + 1)\n",
    "    plt.plot(thresh_train_accuracies[i], '-', label='Training accuracy')\n",
    "    plt.plot(thresh_test_accuracies[i], '-', label='Test accuracy')\n",
    "\n",
    "    plt.title('Data Separation Threshold %0.2f' % thresholds[i])\n",
    "    plt.xlabel('# of iterations')\n",
    "    plt.ylabel('Classification accuracy')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(20,20)) \n",
    "\n",
    "num_thresh = len(thresholds)\n",
    "for i in range(num_thresh):\n",
    "    plt.plot(thresh_costs[i], '-', label='Data Separation Threshold %0.2f' % thresholds[i])\n",
    "\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('L2 Loss')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
